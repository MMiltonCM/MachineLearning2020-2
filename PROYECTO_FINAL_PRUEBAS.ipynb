{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PROYECTO_FINAL_PRUEBAS.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ha8fYqabhhZ"
      },
      "source": [
        "#1\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "#en la descripcion sale que los datos nulos estan representados con el \n",
        "#caracter '?' por lo que se especifica en read_cv\n",
        "df = pd.read_csv('https://drive.google.com/uc?id=1YTnA4ikKK6nzd96isNEEVPoP1r_wVdQm&export=download&authuser=0',na_values='?')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHMOHXuxwfgT"
      },
      "source": [
        "#2\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter\n",
        "sns.set(style='white', context='notebook', palette='deep')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yJt7eu0wt5_"
      },
      "source": [
        "pip install catboost\r\n",
        "#instalamos el modelo catboost\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDZYN6VRwzpm"
      },
      "source": [
        "#instalamos el modelo xgboost\r\n",
        "pip install xgboost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uppP463vDIW"
      },
      "source": [
        "#3\n",
        "from sklearn.ensemble import AdaBoostClassifier   # Ensamble AdaBoost para regresion\n",
        "from sklearn.ensemble import GradientBoostingClassifier   # Ensamble Gradient Boosting para regresion\n",
        "from sklearn.ensemble import RandomForestClassifier   # Ensamble RandomForest para regresion\n",
        "from sklearn.ensemble import ExtraTreesClassifier     # Ensamble ExtraTrees para regresion\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression # para usar modelos de regresion logistica  \n",
        "from sklearn.tree import DecisionTreeClassifier     # para usar modelos de tipo arboles de decision\n",
        "from sklearn.neighbors import KNeighborsClassifier  # para modelos  Knn \n",
        "from sklearn.svm import SVC   # para modelos SVM\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "import lightgbm as lgbm\n",
        "\n",
        "from sklearn.model_selection import train_test_split  # para partir los datos en conjuntos de entrenamiento y validacion\n",
        "from sklearn.model_selection import KFold  # para partir la data en k-folds\n",
        "from sklearn.model_selection import cross_val_score   # para evaluar algoritmos en cross validacion \n",
        "\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score   # para manejar metricas de desempe単o \n",
        "from sklearn.metrics import classification_report  # para hacer reportes de resultados de clasificacion\n",
        "from sklearn.metrics import confusion_matrix  # para manejar matrices de confusion\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeeREuQDf6f9"
      },
      "source": [
        "df.head()\r\n",
        "#exploramos los que tipos de datos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8XcfLF3C9MH"
      },
      "source": [
        "df.shape\r\n",
        "#el tama単o original del dataset descargado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2QPWsFVUmJM"
      },
      "source": [
        "df.isnull().sum()\r\n",
        "#Buscamos datos nulos en las columnas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_x_yJvlqP3k"
      },
      "source": [
        "df=df.drop_duplicates()\r\n",
        "#eliminamos las filas duplicadas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CyfRgdyxVl8"
      },
      "source": [
        "df.shape\r\n",
        "#el nuevo tama単o del dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kggq-ibxCIr"
      },
      "source": [
        "for col in ['workclass', 'occupation', 'native.country']:\n",
        "    df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "#como obervasmos los datos nulos pertenecen a caracteriticos con predominanates\n",
        "#reemplazamos los valores por la moda de cada uno"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2Uf35WE0ZYC"
      },
      "source": [
        "df.isnull().sum()\r\n",
        "#verificamos que no existan valores nulos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGb4aPcYoLmC"
      },
      "source": [
        "df.describe\r\n",
        "#descripcion caracteristica de las variables numericas\r\n",
        "#aca colocar todas las graficas\r\n",
        "#y tambien el outlier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpjIEVFixqdv"
      },
      "source": [
        "#A continuacion se mostraran graficas para e"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKHrt_ONnjsL"
      },
      "source": [
        "##Columnas categoricas y numericas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRKPKJKr0pFf"
      },
      "source": [
        "col_numericas = df.select_dtypes('number').columns\n",
        "col_categoricas = df.select_dtypes(['category','object']).columns\n",
        "#separamos las variables nuemricas de la categoricas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "do3JLLvL0qBs"
      },
      "source": [
        "col_categoricas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jc9fD7Q0r42"
      },
      "source": [
        "col_numericas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ORESq4s1hTp"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV7KlwOb1SMZ"
      },
      "source": [
        "for cate in col_categoricas:\n",
        "  label_encoder.fit(df[cate])\n",
        "  df[cate] = label_encoder.transform(df[cate])\n",
        "#usamos label encoder para convertir las valeriables categoricas a numericas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbI7gneCMhPX"
      },
      "source": [
        "col_numericas = df.select_dtypes('number').columns\n",
        "col_categoricas = df.select_dtypes(['category','object']).columns\n",
        "#verificamos que no hayan variables categoricas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhmosWMbMiSB"
      },
      "source": [
        "col_categoricas\r\n",
        "#se verifica"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Etx2LjiqMkRn"
      },
      "source": [
        "col_numericas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJmP5hmH09xI"
      },
      "source": [
        "df.drop(columns=['fnlwgt'], inplace=True)\r\n",
        "#como no se tiene informacion de esta columna se procede a eliminar\r\n",
        "#ademas de que en la literatura tambien la eliminan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5yW58_IIb0y"
      },
      "source": [
        "# df.drop(columns=['sex'], inplace=True)\r\n",
        "# df.drop(columns=['race'], inplace=True\r\n",
        "# df.drop(columns=['native.country'], inplace=True)\r\n",
        "# se penso eliminar estas columnas, sin embargo el score \r\n",
        "# disminuia cuando se eliminaban , por ello se dejar9on"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RTEy7ZNMmct"
      },
      "source": [
        "df.shape\r\n",
        "# el tama単o de dataset con una columna eliminda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW6HBBjjo5_1"
      },
      "source": [
        "##Division de la data en train y test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aO12UHS92gK_"
      },
      "source": [
        "X_all = df.drop('income',axis=1)\n",
        "#dropeamos la columna con la variable a predecir (target)\n",
        "y_all = df['income']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ-JZA6l2s38"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.20, random_state=123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUe46jobpE6h"
      },
      "source": [
        "##Comparacion de los principales modelos de clasificacion\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCAKYzjFu-ih"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "pipelines = []\n",
        "pipelines.append(('LR', make_pipeline(StandardScaler(), LogisticRegression(solver='liblinear'))))\n",
        "pipelines.append(('DecisionTreeClassifier', make_pipeline(StandardScaler(), DecisionTreeClassifier(max_depth=6))))\n",
        "pipelines.append(('KNN', make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=5))))\n",
        "pipelines.append(('SVM', make_pipeline(StandardScaler(), SVC())))\n",
        "pipelines.append(('AdaBoost', make_pipeline(StandardScaler(), AdaBoostClassifier())))\n",
        "pipelines.append(('GradientBoosting', make_pipeline(StandardScaler(), GradientBoostingClassifier())))\n",
        "pipelines.append(('RandomForest', make_pipeline(StandardScaler(), RandomForestClassifier())))\n",
        "pipelines.append(('ExtraTrees', make_pipeline(StandardScaler(), ExtraTreesClassifier())))\n",
        "pipelines.append(('XGBClassifier', make_pipeline(StandardScaler(), XGBClassifier())))\n",
        "pipelines.append(('lgbm.LGBMClassifier', make_pipeline(StandardScaler(), lgbm.LGBMClassifier(silent=False))))\n",
        "pipelines.append(('CatBoostClassifier', make_pipeline(StandardScaler(), CatBoostClassifier(silent=True))))\n",
        "# Se usaron los modelos y ensambles de clasificacion vistos en el surso,\n",
        "# asi como ensamble no vistos como ExtraTrees,XGBClassifier,lgbm.LGBMClassifier,CatBoostClassifier, ya que \n",
        "# se usaban frecuentemente en modelos de clasificacion binaria "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FLOSh9e23hc"
      },
      "source": [
        "results = []\n",
        "names = []\n",
        "\n",
        "#particion en 10-folds para el cross-validation\n",
        "seed = 7   \n",
        "kfold = KFold(n_splits=10, random_state=seed, shuffle= True)  \n",
        "\n",
        "#  evalua cada pipeline en cross-validation\n",
        "for pipelinename, pipeline in pipelines:\n",
        "    cv_results = cross_val_score(pipeline, X_train, y_train, cv=kfold, scoring='accuracy')\n",
        "    results.append(cv_results)\n",
        "    names.append(pipelinename)\n",
        "    print(\"{}: {} ({})\".format(pipelinename, cv_results.mean(), cv_results.std()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be9V1q_430IQ"
      },
      "source": [
        "fig = plt.figure()\n",
        "fig.suptitle('Accuracy  obtenidas en 10-fold-CV')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names,rotation=90)\n",
        "plt.show()\n",
        "# Se grafican los accuracys obtenidos en un box-plot para identificar los mejores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B29UsK4h1k_p"
      },
      "source": [
        "\r\n",
        "#se hace un nuevo pipeline de los mejores ensables para poder observar\r\n",
        "#mejor su desempe単o\r\n",
        "pipelines = []\r\n",
        "pipelines.append(('AdaBoost', make_pipeline(StandardScaler(), AdaBoostClassifier())))\r\n",
        "pipelines.append(('GradientBoosting', make_pipeline(StandardScaler(), GradientBoostingClassifier())))\r\n",
        "pipelines.append(('XGBClassifier', make_pipeline(StandardScaler(), XGBClassifier())))\r\n",
        "pipelines.append(('lgbm.LGBMClassifier', make_pipeline(StandardScaler(), lgbm.LGBMClassifier(silent=False))))\r\n",
        "pipelines.append(('CatBoostClassifier', make_pipeline(StandardScaler(), CatBoostClassifier(silent=True))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgprTjW41ujD"
      },
      "source": [
        "# Se usa de nuevo cross-validation\r\n",
        "results = []\r\n",
        "names = []\r\n",
        "\r\n",
        "\r\n",
        "seed = 7   \r\n",
        "kfold = KFold(n_splits=10, random_state=seed, shuffle= True)  # especifica el particionador de datos a 10-folds CV\r\n",
        "\r\n",
        "#  evalua cada pipeline en crosvalidacion\r\n",
        "for pipelinename, pipeline in pipelines:\r\n",
        "    cv_results = cross_val_score(pipeline, X_train, y_train, cv=kfold, scoring='accuracy')\r\n",
        "    results.append(cv_results)\r\n",
        "    names.append(pipelinename)\r\n",
        "    print(\"{}: {} ({})\".format(pipelinename, cv_results.mean(), cv_results.std()))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}